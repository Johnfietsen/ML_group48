{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.optimizers import Adam, Adadelta\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten, Conv1D\n",
    "#from keras.layers.convolutional import MaxPooling2D, MaxPooling1D\n",
    "#from keras.models import load_model\n",
    "from time import time, strftime\n",
    "import theano\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne import nonlinearities\n",
    "from lasagne import init\n",
    "#from lasagne.layers import cuda_convnet\n",
    "from lasagne.layers import conv\n",
    "from lasagne.layers import pool\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_train(subj_id):\n",
    "    data_list, events_list = [], []\n",
    "    for series_id in range(1, 2):\n",
    "        fname_data = 'C:/Users/Gebruiker/Documents/Untitled Folder/train/train/subj%d_series%d_data.csv' % (\n",
    "            subj_id, series_id)\n",
    "        fname_events = 'C:/Users/Gebruiker/Documents/Untitled Folder/train/train/subj%d_series%d_events.csv' % (\n",
    "            subj_id, series_id)\n",
    "\n",
    "        data = pd.read_csv(fname_data)[:10000]\n",
    "        events = pd.read_csv(fname_events)[:10000]\n",
    "        channel_names = data.columns[1:]\n",
    "        data_list.append(data[channel_names].values.T)\n",
    "\n",
    "        event_names = events.columns[1:]\n",
    "        events_list.append(events[event_names].values.T.astype(np.int32))\n",
    "\n",
    "    return data_list, events_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(data_list, events_list, val_size=2, rand=False):\n",
    "    # randomly choose val_size time series for validation\n",
    "    if rand:\n",
    "        val_ind = np.random.choice(8, size=val_size, replace=False)\n",
    "    # just use the last two time series for validation\n",
    "    else:\n",
    "        val_ind = np.arange(8 - val_size, 8)\n",
    "\n",
    "    train_data, valid_data = [], []\n",
    "    train_events, valid_events = [], []\n",
    "    print(len(data_list))\n",
    "    # separate the time series into training and validation\n",
    "    for i in range(1):\n",
    "        if i == 0:\n",
    "            train_data.append(data_list[i])\n",
    "            train_events.append(events_list[i])\n",
    "        else:\n",
    "            valid_data.append(data_list[i])\n",
    "            valid_events.append(events_list[i])\n",
    "\n",
    "    return train_data, train_events, valid_data, valid_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute the time-windows of all time-series to\n",
    "# give a shuffled list of time-windows over all\n",
    "# time series\n",
    "def get_permuted_windows(series_list, window_size, rand=True):\n",
    "    series_slices = []\n",
    "    for i, series in enumerate(series_list):\n",
    "        slices = get_series_window_slices(series.shape[1],\n",
    "                                          window_size)\n",
    "        # need to mark each slice with the series it came from\n",
    "        for s in slices:\n",
    "            series_slices.append((i, s))\n",
    "\n",
    "    # wish to iterate over the windows in random order for training\n",
    "    if rand:\n",
    "        random.shuffle(series_slices)\n",
    "    return series_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_window_slices(num_datapoints, window_size):\n",
    "    slices = []\n",
    "    num_windows = num_datapoints - window_size + 1\n",
    "    for i in range(0, num_windows, 1):\n",
    "        slices.append(slice(i, i + window_size))\n",
    "\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iter_funcs_train():\n",
    "    X = K.placeholder(ndim=3)\n",
    "    Y = K.placeholder(ndim=2)\n",
    "    x_batch = K.placeholder(ndim=3)\n",
    "    y_batch = K.placeholder(ndim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(bs, W, X, y=None, noisy=False):\n",
    "    if not W:\n",
    "        raise StopIteration\n",
    "    window_size = W[0][1].stop - W[0][1].start\n",
    "    # total number of batches for this data set and batch size\n",
    "    N = int((len(W) + bs - 1) / bs)\n",
    "    for i in range(N):\n",
    "        Wb = W[i * bs:(i + 1) * bs]\n",
    "\n",
    "        X_batch_list, y_batch_list = [], []\n",
    "        # index: which time series to take the window from\n",
    "        # s:     the slice to take from that time series\n",
    "        for j, (index, s) in enumerate(Wb):\n",
    "            X_window = X[index][:, s]\n",
    "            if y is not None:\n",
    "                y_window = y[index][:, s][:, -1]\n",
    "                y_batch_list.append(y_window)\n",
    "\n",
    "            # this is test data, train data but no action is present,\n",
    "            # or validation data (don't want noise)\n",
    "            if y is None or y_window.sum() == 0 or not noisy:\n",
    "                X_batch_list.append(X_window)\n",
    "            # this is train data and an action is present, so add noise\n",
    "            else:\n",
    "                noise = np.random.normal(\n",
    "                    0, 0.1, X_window.shape).astype(np.float32)\n",
    "                X_batch_list.append(X_window + noise)\n",
    "\n",
    "        # reshape to (batch_size, num_channels, window_size)\n",
    "        X_batch = np.vstack(X_batch_list).reshape(-1,\n",
    "                                                  X[0].shape[0], window_size)\n",
    "        if not y_batch_list:\n",
    "            y_batch = None\n",
    "        else:\n",
    "            y_batch = np.vstack(y_batch_list)\n",
    "\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size,num_channels,input_length,output_shape):\n",
    "    model = Sequential((\n",
    "    # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "    # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "    # the input timeseries, the activation of each filter at that position.\n",
    "    Conv1D(nb_filter=8, filter_length=3, activation='relu', input_shape=(batch_size,num_channels)),\n",
    "    MaxPooling1D(3,2),     # Downsample the output of convolution by 2X.\n",
    "    Conv1D(nb_filter=16, filter_length=3, activation='relu'),\n",
    "    MaxPooling1D(3,2),\n",
    "    Dropout(0.5),\n",
    "    Dense(32),\n",
    "    Dense(output_shape),    \n",
    "    ))\n",
    "    optimizer = Adadelta()\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lasagne(batch_size,\n",
    "                num_channels,\n",
    "                input_length,\n",
    "                output_dim,):\n",
    "    Conv1DLayer = conv.Conv1DLayer\n",
    "    MaxPool1DLayer = pool.MaxPool1DLayer\n",
    "    l_in = layers.InputLayer(\n",
    "        shape=(batch_size, num_channels, input_length),\n",
    "        name='l_in',\n",
    "    )\n",
    "\n",
    "    l_conv1 = Conv1DLayer(\n",
    "        l_in,\n",
    "        name='conv1',\n",
    "        num_filters=8,\n",
    "        filter_size=3,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        W=init.Orthogonal(),\n",
    "    )\n",
    "\n",
    "    l_pool1 = MaxPool1DLayer(\n",
    "        l_conv1,\n",
    "        name='pool1',\n",
    "        pool_size=3,\n",
    "        stride=2,\n",
    "    )\n",
    "\n",
    "    l_conv2 = Conv1DLayer(\n",
    "        l_pool1,\n",
    "        name='conv2',\n",
    "        num_filters=16,\n",
    "        filter_size=3,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        W=init.Orthogonal(),\n",
    "    )\n",
    "\n",
    "    l_pool2 = MaxPool1DLayer(\n",
    "        l_conv2,\n",
    "        name='pool2',\n",
    "        pool_size=3,\n",
    "        stride=2,\n",
    "    )\n",
    "\n",
    "    l_dropout_dense1 = layers.DropoutLayer(\n",
    "        #l_pool4,\n",
    "        l_pool2,\n",
    "        p=0.5,\n",
    "    )\n",
    "\n",
    "    l_dense1 = layers.DenseLayer(\n",
    "        l_dropout_dense1,\n",
    "        num_units=32,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        W=init.Orthogonal(),\n",
    "    )\n",
    "\n",
    "    l_out = layers.DenseLayer(\n",
    "        l_dense1,\n",
    "        num_units=output_dim,\n",
    "        nonlinearity=nonlinearities.sigmoid,\n",
    "        W=init.Orthogonal(),\n",
    "    )\n",
    "\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    return strftime('%Y-%m-%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iter_funcs_train_theano(lr, mntm, l_out):\n",
    "    X = T.tensor3('x')\n",
    "    y = T.imatrix('y')\n",
    "    X_batch = T.tensor3('x_batch')\n",
    "    y_batch = T.imatrix('y_batch')\n",
    "\n",
    "    train_output = layers.get_output(l_out, X_batch)\n",
    "    train_loss = T.mean(T.nnet.binary_crossentropy(train_output, y_batch))\n",
    "\n",
    "    all_params = layers.get_all_params(l_out)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        train_loss, all_params, lr, mntm)\n",
    "\n",
    "    train_iter = theano.function(\n",
    "        inputs=[theano.Param(X_batch),\n",
    "                theano.Param(y_batch)],\n",
    "        outputs=[train_loss, train_output],\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "        },\n",
    "    )\n",
    "    return train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iter_funcs_val_theano(l_out):\n",
    "    X = T.tensor3('x')\n",
    "    y = T.imatrix('y')\n",
    "    X_batch = T.tensor3('x_batch')\n",
    "    y_batch = T.imatrix('y_batch')\n",
    "\n",
    "    valid_output = layers.get_output(l_out, X_batch, deterministic=True)\n",
    "    valid_loss = T.mean(T.nnet.binary_crossentropy(valid_output, y_batch))\n",
    "\n",
    "    valid_iter = theano.function(\n",
    "        inputs=[theano.Param(X_batch),\n",
    "                theano.Param(y_batch)],\n",
    "        outputs=[valid_loss, valid_output],\n",
    "        givens={\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return valid_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(window_size, max_epochs, patience):\n",
    "    #root_dir = join('data', 'nets')\n",
    "    # the file from which to load pre-trained weights\n",
    "    #init_file = join(root_dir,\n",
    "    #                 'subj%d_weights_deep_nocsp_wide.pickle' % (\n",
    "    #                     4))\n",
    "    #init_file = join(root_dir,\n",
    "    #                 'weights_super_deeper.pickle')\n",
    "    init_file = None\n",
    "    # the file to which the learned weights will be written\n",
    "    #weights_file = join(root_dir,\n",
    "                        #'weights.pickle')\n",
    "    #temp_weights_file = join(root_dir, 'epoch_%d.pickle')\n",
    "    train_data, train_events = [], []\n",
    "    valid_data, valid_events = [], []\n",
    "    for subj_id in range(1, 2):\n",
    "        print('loading time series for subject %d...' % (subj_id))\n",
    "        subj_data_list, subj_events_list = load_subject_train(subj_id)\n",
    "        print('  creating train and validation sets...')\n",
    "        subj_train_data, subj_train_events, subj_valid_data, subj_valid_events = \\\n",
    "            split_train_test_data(subj_data_list, subj_events_list,\n",
    "                                        val_size=2, rand=False)\n",
    "        train_data += subj_train_data\n",
    "        train_events += subj_train_events\n",
    "        valid_data += subj_valid_data\n",
    "        valid_events += subj_valid_events\n",
    "\n",
    "    print('using %d time series for training' % (len(train_data)))\n",
    "    print('using %d time series for validation' % (len(valid_data)))\n",
    "\n",
    "    print('creating fixed-size time-windows of size %d' % (window_size))\n",
    "    # the training windows should be in random order\n",
    "    train_slices = get_permuted_windows(train_data, window_size,\n",
    "                                                 rand=True)\n",
    "    valid_slices = get_permuted_windows(valid_data, window_size,\n",
    "                                                 rand=True)\n",
    "    print('there are %d windows for training' % (len(train_slices)))\n",
    "    print('there are %d windows for validation' % (len(valid_slices)))\n",
    "\n",
    "    #batch_size = 64\n",
    "    batch_size = 512\n",
    "    num_channels = 32\n",
    "    num_actions = 6\n",
    "    #train_data, valid_data = \\\n",
    "        #utils.preprocess(train_data, valid_data)\n",
    "\n",
    "    #print('building model %s...' % (\n",
    "        #sys.modules[build_model.__module__].__name__))\n",
    "    l_out = build_model_lasagne(None, num_channels,\n",
    "                        window_size, num_actions)\n",
    "    print(l_out)\n",
    "    #all_layers = layers.get_all_layers(l_out)\n",
    "    #print('this network has %d learnable parameters' %\n",
    "          #(layers.count_params(l_out)))\n",
    "    #for layer in all_layers:\n",
    "        #print('Layer %s has output shape %r' %\n",
    "              #(layer.name, layer.output_shape))\n",
    "\n",
    "    if init_file is not None:\n",
    "        print('loading model weights from %s' % (init_file))\n",
    "        with open(init_file, 'rb') as ifile:\n",
    "            src_layers = pickle.load(ifile)\n",
    "        dst_layers = layers.get_all_params(l_out)\n",
    "        for i, (src_weights, dst_layer) in enumerate(\n",
    "                zip(src_layers, dst_layers)):\n",
    "            print('loading pretrained weights for %s' % (dst_layer.name))\n",
    "            dst_layer.set_value(src_weights)\n",
    "    else:\n",
    "        print('all layers will be trained from random initialization')\n",
    "\n",
    "    #1r = theano.shared(np.cast['float32'](0.001))\n",
    "    lr = theano.shared(np.cast['float32'](0.01))\n",
    "    mntm = 0.9\n",
    "    print('compiling theano functions...')\n",
    "    train_iter = create_iter_funcs_train_theano(lr, mntm, l_out)\n",
    "    valid_iter = create_iter_funcs_val_theano(l_out)\n",
    "\n",
    "    best_weights = None\n",
    "    best_valid_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    print('starting training for all subjects at %s' % (\n",
    "        get_current_time()))\n",
    "    best_weights = None\n",
    "    best_valid_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    print('starting training for all subjects')\n",
    "    try:\n",
    "        for epoch in range(max_epochs):\n",
    "            print('epoch: %d' % (epoch))\n",
    "            train_losses, training_outputs, training_inputs = [], [], []\n",
    "            num_batches = (len(train_slices) + batch_size - 1) / batch_size\n",
    "            t_train_start = time()\n",
    "            for i, (Xb, yb) in enumerate(\n",
    "                batch_iterator(batch_size,\n",
    "                                        train_slices,\n",
    "                                        train_data,\n",
    "                                        train_events)):\n",
    "                t_batch_start = time()\n",
    "                # hack for faster debugging\n",
    "                #if i < 70000:\n",
    "                #    continue\n",
    "                train_loss, train_output = \\\n",
    "                    train_iter(Xb, yb)\n",
    "                if np.isnan(train_loss):\n",
    "                    print('nan loss encountered in minibatch %d' % (i))\n",
    "                    continue\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                assert len(yb) == len(train_output)\n",
    "                for input, output in zip(yb, train_output):\n",
    "                    training_inputs.append(input)\n",
    "                    training_outputs.append(output)\n",
    "                \n",
    "                batch_duration = time() - t_batch_start\n",
    "                if i % 10 == 0:\n",
    "                    eta = batch_duration * (num_batches - i)\n",
    "                    m, s = divmod(eta, 60)\n",
    "                    h, m = divmod(m, 60)\n",
    "                    print('  training...  (ETA = %d:%02d:%02d)\\r'\n",
    "                          % (h, m, s)),\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            training_inputs = np.vstack(training_inputs)\n",
    "            training_outputs = np.vstack(training_outputs)\n",
    "            try:\n",
    "                train_roc = roc_auc_score(training_inputs, training_outputs)\n",
    "            except ValueError:\n",
    "                train_roc = 0\n",
    "                pass\n",
    "\n",
    "            train_duration = time() - t_train_start\n",
    "            print('')\n",
    "            print('    train loss: %.6f' % (avg_train_loss))\n",
    "            print('    train roc:  %.6f' % (train_roc))\n",
    "            print('    duration:   %.2f s' % (train_duration))\n",
    "\n",
    "            valid_losses, valid_outputs, valid_inputs = [], [], []\n",
    "            num_batches = (len(valid_slices) + batch_size - 1) / batch_size\n",
    "            t_valid_start = time()\n",
    "            for i, (Xb, yb) in enumerate(\n",
    "                batch_iterator(batch_size,\n",
    "                                        valid_slices,\n",
    "                                        valid_data,\n",
    "                                        valid_events)):\n",
    "                t_batch_start = time()\n",
    "                valid_loss, valid_output = \\\n",
    "                    valid_iter(Xb, yb)\n",
    "                if np.isnan(valid_loss):\n",
    "                    print('nan loss encountered in minibatch %d' % (i))\n",
    "                    continue\n",
    "                valid_losses.append(valid_loss)\n",
    "                assert len(yb) == len(valid_output)\n",
    "                for input, output in zip(yb, valid_output):\n",
    "                    valid_inputs.append(input)\n",
    "                    valid_outputs.append(output)\n",
    "\n",
    "                batch_duration = time() - t_batch_start\n",
    "                if i % 10 == 0:\n",
    "                    eta = batch_duration * (num_batches - i)\n",
    "                    m, s = divmod(eta, 60)\n",
    "                    h, m = divmod(m, 60)\n",
    "                    #print('  validation...  (ETA = %d:%02d:%02d)\\r'\n",
    "                         # % (h, m, s)),\n",
    "\n",
    "            # allow training without validation\n",
    "            if valid_losses:\n",
    "                avg_valid_loss = np.mean(valid_losses)\n",
    "                valid_inputs = np.vstack(valid_inputs)\n",
    "                valid_outputs = np.vstack(valid_outputs)\n",
    "                try:\n",
    "                    valid_roc = roc_auc_score(valid_inputs, valid_outputs)\n",
    "                except ValueError:\n",
    "                    valid_roc = 0\n",
    "                    pass\n",
    "                valid_duration = time() - t_valid_start\n",
    "                print('')\n",
    "                print('    valid loss: %.6f' % (avg_valid_loss))\n",
    "                print('    valid roc:  %.6f' % (valid_roc))\n",
    "                print('    duration:   %.2f s' % (valid_duration))\n",
    "            else:\n",
    "                print('    no validation...')\n",
    "\n",
    "            # if we are not doing validation we always want the latest weights\n",
    "            if not valid_losses:\n",
    "                best_epoch = epoch\n",
    "                model_train_loss = avg_train_loss\n",
    "                model_train_roc = train_roc\n",
    "                model_valid_roc = -1.\n",
    "                best_valid_loss = -1.\n",
    "                best_weights = layers.get_all_param_values(l_out)\n",
    "            elif avg_valid_loss < best_valid_loss:\n",
    "                best_epoch = epoch\n",
    "                model_train_roc = train_roc\n",
    "                model_valid_roc = valid_roc\n",
    "                model_train_loss = avg_train_loss\n",
    "                best_valid_loss = avg_valid_loss\n",
    "                best_weights = layers.get_all_param_values(l_out)\n",
    "\n",
    "                #temp_file = temp_weights_file % (epoch)\n",
    "                #print('saving temporary best weights to %s' % (temp_file))\n",
    "                #with open(temp_file, 'wb') as ofile:\n",
    "                    #pickle.dump(best_weights, ofile,\n",
    "                               # protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            if epoch > best_epoch + patience:\n",
    "                break\n",
    "                best_epoch = epoch\n",
    "                new_lr = 0.5 * lr.get_value()\n",
    "                lr.set_value(np.cast['float32'](new_lr))\n",
    "                print('setting learning rate to %.6f' % (new_lr))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('caught Ctrl-C, stopping training...')\n",
    "\n",
    "    #with open(weights_file, 'wb') as ofile:\n",
    "        #print('saving best weights to %s' % (weights_file))\n",
    "        #pickle.dump(best_weights, ofile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('finished training for all subjects at')\n",
    "\n",
    "    return model_train_loss, best_valid_loss, model_train_roc, model_valid_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(roc_auc_score([[0,0,0][1,1,1]],[[0,0,0][1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading time series for subject 1...\n",
      "  creating train and validation sets...\n",
      "1\n",
      "using 1 time series for training\n",
      "using 0 time series for validation\n",
      "creating fixed-size time-windows of size 200\n",
      "there are 9801 windows for training\n",
      "there are 0 windows for validation\n",
      "<lasagne.layers.dense.DenseLayer object at 0x00000224484E9E80>\n",
      "all layers will be trained from random initialization\n",
      "compiling theano functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\lasagne\\theano_extensions\\conv.py:66: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "  border_mode=border_mode)\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\lasagne\\layers\\pool.py:142: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  padding=(self.pad[0], 0),\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\lasagne\\layers\\pool.py:142: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  padding=(self.pad[0], 0),\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\lasagne\\layers\\pool.py:142: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  padding=(self.pad[0], 0),\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\theano\\tensor\\nnet\\conv.py:98: UserWarning: theano.tensor.nnet.conv.conv2d is deprecated. Use theano.tensor.nnet.conv2d instead.\n",
      "  warnings.warn(\"theano.tensor.nnet.conv.conv2d is deprecated.\"\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training for all subjects at 2018-03-12_17:28:08\n",
      "starting training for all subjects\n",
      "epoch: 0\n",
      "  training...  (ETA = 0:17:34)\n",
      "  training...  (ETA = 0:09:40)\n",
      "\n",
      "    train loss: 15.570708\n",
      "    train roc:  0.497541\n",
      "    duration:   1079.23 s\n",
      "    no validation...\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:19:29)\n",
      "  training...  (ETA = 0:10:56)\n",
      "\n",
      "    train loss: 0.397077\n",
      "    train roc:  0.490750\n",
      "    duration:   1126.78 s\n",
      "    no validation...\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:18:27)\n",
      "  training...  (ETA = 0:09:01)\n",
      "\n",
      "    train loss: 0.362026\n",
      "    train roc:  0.496357\n",
      "    duration:   1046.43 s\n",
      "    no validation...\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:18:04)\n",
      "  training...  (ETA = 0:09:12)\n",
      "\n",
      "    train loss: 0.328766\n",
      "    train roc:  0.496357\n",
      "    duration:   1032.95 s\n",
      "    no validation...\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:18:13)\n",
      "  training...  (ETA = 0:09:01)\n",
      "\n",
      "    train loss: 0.304696\n",
      "    train roc:  0.487388\n",
      "    duration:   1031.34 s\n",
      "    no validation...\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:18:09)\n",
      "  training...  (ETA = 0:08:58)\n",
      "\n",
      "    train loss: 0.284673\n",
      "    train roc:  0.487657\n",
      "    duration:   1023.04 s\n",
      "    no validation...\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:18:22)\n",
      "  training...  (ETA = 0:09:08)\n",
      "\n",
      "    train loss: 0.267854\n",
      "    train roc:  0.485760\n",
      "    duration:   1031.18 s\n",
      "    no validation...\n",
      "epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:17:52)\n",
      "  training...  (ETA = 0:09:02)\n",
      "\n",
      "    train loss: 0.253882\n",
      "    train roc:  0.488275\n",
      "    duration:   1021.86 s\n",
      "    no validation...\n",
      "epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:17:25)\n",
      "  training...  (ETA = 0:09:08)\n",
      "\n",
      "    train loss: 0.242395\n",
      "    train roc:  0.486067\n",
      "    duration:   1018.97 s\n",
      "    no validation...\n",
      "epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training...  (ETA = 0:17:59)\n",
      "  training...  (ETA = 0:08:45)\n",
      "\n",
      "    train loss: 0.232717\n",
      "    train roc:  0.487461\n",
      "    duration:   1021.11 s\n",
      "    no validation...\n",
      "finished training for all subjects at\n",
      "(0.2327168373720523, -1.0, 0.4874612925089335, -1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: DeprecationWarning: generator 'batch_iterator' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "print(train_model(200,10,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
