{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten, Conv1D\n",
    "from keras.layers.convolutional import MaxPooling2D, MaxPooling1D\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants:\n",
    "SUBSAMPLE = 100  # scales the data by this factor can be replaced by divding the steps per epoch by batch size\n",
    "COLUMNS = ['HandStart', 'FirstDigitTouch',\n",
    "        'BothStartLoadPhase', 'LiftOff',\n",
    "        'Replace', 'BothReleased']\n",
    "SUBJECTS = range(1, 2)\n",
    "N_LABELS = 6\n",
    "\n",
    "# Below are the paths to the data. Please pay attention to the % and * signs, these are needed in the loop.\n",
    "#TRAIN_DATA_PATH = 'C:/Users/Sebastiaan/Desktop/Programming/MachineLearning/Datasets/EEG/train/subj%d_series*_data.csv'\n",
    "TRAIN_DATA_PATH = 'C:/Users/Gebruiker/Documents/Untitled Folder/train/train/subj%d_series*_data.csv'\n",
    "#The path below is for the test data used for a kaggle submission. This is not very relevant to our project.\n",
    "# TEST_DATA_PATH = 'C:/Users/Sebastiaan/Desktop/Programming/MachineLearning/Datasets/EEG/test/subj%d_series*_data.csv'\n",
    "# TRAIN_DATA_PATH = 'C:/Users/bas/Documents/MachineLearning/train/subj%d_series*_data.csv' #path on my laptop\n",
    "# TEST_DATA_PATH =  'C:/Users/bas/Documents/MachineLearning/test/subj%d_series*_data.csv' \n",
    "\n",
    "SUBMISSION_FOLDER = 'C:/Users/Sebastiaan/Desktop/Programming/MachineLearning/'\n",
    "SUBMISSION_NAME = 'subbmision_vu_48_sub_pca_4.csv'\n",
    "\n",
    "PCA_COMPONENTS = 0.8\n",
    "CUTT_OFF_FREQUENCY = 2\n",
    "ORDER = 4\n",
    "SAMPLE_FREQUENCY = 500\n",
    "\n",
    "EPOCHS = 5\n",
    "WINDOW_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_data():\n",
    "    data = pd.read_csv(\"C:/Users/Gebruiker/Documents/Untitled Folder/train/train/subj1_series1_data.csv\")\n",
    "    labels = pd.read_csv(\"C:/Users/Gebruiker/Documents/Untitled Folder/train/train/subj1_series1_events.csv\")\n",
    "    data = data.drop(['id'], axis = 1)\n",
    "    data['HandStart'] = labels[\"HandStart\"]\n",
    "    data['FirstDigitTouch'] = labels[\"FirstDigitTouch\"]\n",
    "    data['BothStartLoadPhase'] = labels[\"BothStartLoadPhase\"]\n",
    "    data['LiftOff'] = labels[\"LiftOff\"]\n",
    "    data['Replace'] = labels[\"Replace\"]\n",
    "    data['BothReleased'] = labels[\"BothReleased\"]\n",
    "    data = np.asarray(data[4500:5500].astype(float))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    event_path = data_path.replace('_data', '_events')\n",
    "    labels = pd.read_csv(event_path)\n",
    "    clean_data = data.drop(['id'], axis = 1)\n",
    "    labels = labels.drop(['id'], axis = 1)\n",
    "    return clean_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_transform(data, scaler):\n",
    "    if scaler == None:\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(data), scaler\n",
    "    else:\n",
    "        return scaler.transform(data)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(train_data_paths):\n",
    "    labels_raw = []\n",
    "    features_raw = []\n",
    "    for data_path in train_data_paths:\n",
    "        data, labels = prepare_training_data(data_path)\n",
    "        features_raw.append(data[1722:2071])\n",
    "        labels_raw.append(labels[1722:2071])\n",
    "    features_raw = pd.concat(features_raw)\n",
    "    labels_raw = pd.concat(labels_raw)\n",
    "    x_train = np.asarray(features_raw.astype(float))\n",
    "    y_train = np.asarray(labels_raw.astype(float))\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data(test_data_paths):\n",
    "    test_features_raw = []\n",
    "    ids = []\n",
    "    for data_path in test_data_paths:\n",
    "        data = prepare_test_data(data_path)\n",
    "        test_features_raw.append(data)\n",
    "        ids.append(np.array(data['id']))\n",
    "    test_features_raw = pd.concat(test_features_raw)\n",
    "    ids = np.concatenate(ids)\n",
    "    test_features_raw = test_features_raw.drop(['id'], axis = 1)\n",
    "    x_test = np.asarray(test_features_raw.astype(float))\n",
    "    return x_test, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_auc_score(predictions, y_test, with_plot):\n",
    "    scores = []\n",
    "    legend_text = []\n",
    "    for i in range(N_LABELS):\n",
    "        fpr, tpr, _  = roc_curve(y_test[:,i], predictions[:,i], 1)\n",
    "        scores.append(roc_auc_score(y_test[:,i], predictions[:,i]))\n",
    "        legend_text.append(COLUMNS[i]+' (area = %.3f)' % (scores[i]))\n",
    "        if with_plot == True:\n",
    "            plt.plot(fpr, tpr)\n",
    "    if with_plot == True:\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves')\n",
    "        plt.legend(legend_text)\n",
    "        plt.show()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_metric_auc_score(predictions, y_test, with_plot):\n",
    "    fpr, tpr, _  = roc_curve(y_test, predictions, 1)\n",
    "    score = roc_auc_score(y_test, predictions)\n",
    "    print(COLUMNS[0]+' AUC score = %.3f' % (score))\n",
    "    if with_plot == True:\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves')\n",
    "        plt.show()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_auc_scores(prediction_total, test_data_total, subjects, with_plot):\n",
    "    scores = []\n",
    "    for subject in subjects:\n",
    "        score = metric_auc_score(prediction_total[subject-1],\n",
    "                                 test_data_total[subject-1], with_plot)\n",
    "        scores.append(score)\n",
    "        print('Mean AUC Score of Subject %d: %.3f' % \\\n",
    "              (subject, np.mean(score)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_mappping(x_train, WINDOW_SIZE):\n",
    "#     result = []\n",
    "#     empty_matrix = np.atleast_3d(np.zeros(np.shape(x_train[0:WINDOW_SIZE])))\n",
    "#     for i in range(len(x_train)):\n",
    "#         if i-WINDOW_SIZE < 0:\n",
    "#             result.append(empty_matrix)\n",
    "#         else:\n",
    "#             result.append(np.atleast_3d(x_train[i-WINDOW_SIZE:i]))\n",
    "#     return result\n",
    "\n",
    "def image_mapping(x_train, WINDOW_SIZE):\n",
    "    result = []\n",
    "    empty_matrix = np.zeros(np.shape(x_train[0:WINDOW_SIZE]))\n",
    "    for i in range(len(x_train)):\n",
    "        if i-WINDOW_SIZE < 0:\n",
    "            result.append(empty_matrix)\n",
    "        else:\n",
    "            result.append(x_train[i-WINDOW_SIZE:i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(x_train, y_train, WINDOW_SIZE):\n",
    "    x = image_mappping(x_train, WINDOW_SIZE)\n",
    "    while True:\n",
    "        for image, task in zip(x, y_train):\n",
    "            yield np.array([image]), np.array([task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(x_test, WINDOW_SIZE):\n",
    "    x = image_mappping(x_test, WINDOW_SIZE)\n",
    "    while True:\n",
    "        for image in x:\n",
    "            yield np.array([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_val(vals):\n",
    "    new = np.empty([len(vals),1])\n",
    "    for i in range(0,len(vals)):\n",
    "        value = vals[i]\n",
    "        for index in range(0,len(value)):\n",
    "            if value[index] == 1:\n",
    "                new[i] = index+1\n",
    "                break\n",
    "            elif index == 0:\n",
    "                new[i] = 0\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights(y_train):\n",
    "    class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train[:,0]), y_train[:,0])\n",
    "    return {0 : class_weight[1], 2: class_weight[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(comb):\n",
    "    df_majority = np.array([value for value in comb if sum(value[-6:]) == 0])\n",
    "    df_minority = np.array([value for value in comb if sum(value[-6:]) != 0])\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "    df_upsampled = np.concatenate((df_majority, df_minority_upsampled))\n",
    "    return df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(window):\n",
    "    \"\"\"model = Sequential((\n",
    "    # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "    # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "    # the input timeseries, the activation of each filter at that position.\n",
    "    Conv1D(nb_filter=50, filter_length=100, activation='relu', input_shape=(window, 32)),\n",
    "    MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "    Conv1D(nb_filter=50, filter_length=50, activation='relu'),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(5, activation='softmax'),    \n",
    "    ))\n",
    "    optimizer = Adadelta()\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\"\"\"\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size=(2, 2), activation='relu', input_shape=(8,4,1)))\n",
    "    model.add(Conv2D(32, 2,2 , activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25)) # Dropout 25% of the nodes of the previous layer during training\n",
    "    model.add(Flatten())     # Flatten, and add a fully connected layer\n",
    "    model.add(Dense(128, activation='relu')) \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation='sigmoid')) # Last layer: 10 class nodes, with dropout\n",
    "    model.summary()\n",
    "    optimizer = Adadelta()\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1186, 32)\n"
     ]
    }
   ],
   "source": [
    "for subject in SUBJECTS:\n",
    "    test_features_raw = []\n",
    "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
    "    np.set_printoptions(threshold=np.nan)\n",
    "    #x_train, y_train = read_training_data(train_data_paths) \n",
    "    complete_x = get_complete_data()\n",
    "    #y_train = np.random.randint(2,size=len(y_train))\n",
    "    #print(y_train)\n",
    "    complete_resampled = resampling(complete_x)\n",
    "    x = complete_resampled[:,range(0,(complete_resampled.shape[1]-6))]\n",
    "    y = complete_resampled[:, range(-6,-0)]\n",
    "    #print(x)\n",
    "    print(x.shape)\n",
    "    #y_train = np.array([value for value in y_train if sum(value) != 0])\n",
    "    #print(y_train)\n",
    "    #x_train = x_train[0:len(y_train)]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "    #x_train = x_train.reshape(len(x_train),1,8,4)\n",
    "    #x_test = x_test.reshape(len(x_test),1,8,4)\n",
    "    #y_train = to_categorical(y_train, num_classes = 6)\n",
    "    #print(y_train.shape)\n",
    "    #print(image_mapping(x_train,500))\n",
    "    #y_train = simple_val(y_train)\n",
    "    #y_test = simple_val(y_test)\n",
    "    #class_weights = class_weight.compute_class_weight('balanced', set(np.unique(y_train)), y_train)\n",
    "    #print(class_weights)\n",
    "    #class_weights = {0 : 1., 1: 50., 2: 50.,3: 50., 4: 50.,5: 50.}\n",
    "    \n",
    "    #print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 7, 3, 16)          80        \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 6, 2, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               12416     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 15,350\n",
      "Trainable params: 15,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = init_cnn(window=593)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494 samples, validate on 99 samples\n",
      "Epoch 1/10\n",
      "494/494 [==============================] - 1s 2ms/step - loss: 4.4602 - acc: 0.7021 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 2/10\n",
      "494/494 [==============================] - 0s 97us/step - loss: 2.3232 - acc: 0.8489 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 3/10\n",
      "494/494 [==============================] - 0s 97us/step - loss: 2.0910 - acc: 0.8674 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 4/10\n",
      "494/494 [==============================] - 0s 96us/step - loss: 2.0555 - acc: 0.8708 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 5/10\n",
      "494/494 [==============================] - 0s 97us/step - loss: 2.0240 - acc: 0.8728 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 6/10\n",
      "494/494 [==============================] - 0s 89us/step - loss: 1.9921 - acc: 0.8748 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 7/10\n",
      "494/494 [==============================] - 0s 89us/step - loss: 2.0090 - acc: 0.8748 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 8/10\n",
      "494/494 [==============================] - 0s 87us/step - loss: 2.0025 - acc: 0.8745 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 9/10\n",
      "494/494 [==============================] - 0s 86us/step - loss: 1.9625 - acc: 0.8772 - val_loss: 1.9266 - val_acc: 0.8805\n",
      "Epoch 10/10\n",
      "494/494 [==============================] - 0s 88us/step - loss: 1.9670 - acc: 0.8775 - val_loss: 1.9266 - val_acc: 0.8805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x273187edba8>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(len(x_train),8,4)\n",
    "x_train = x_train[:, :, :, None]\n",
    "model.fit(x_train,y_train,epochs=10, batch_size=30, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 8, 4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate(x_test,y_test)\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "x_test = x_test.reshape(len(x_test),8,4)\n",
    "x_test = x_test[:, :, :, None]\n",
    "print(x_test.shape)\n",
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[902   0   0   0]\n",
      " [134   0   0   0]\n",
      " [ 48   0   0   0]\n",
      " [312   0   0   0]]\n",
      "0.6461318051575932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_compare = np.argmax(y_test,axis=1)\n",
    "score = accuracy_score(y_compare,pred)\n",
    "cm = confusion_matrix(y_compare,pred)\n",
    "print(cm)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "28/27 [==============================] - 1s 39ms/step - loss: -959.3900 - acc: 0.0000e+00 - val_loss: -2.2775 - val_acc: 0.0000e+00\n",
      "Epoch 2/4\n",
      "28/27 [==============================] - 0s 11ms/step - loss: -530.6537 - acc: 0.0000e+00 - val_loss: -6.2631 - val_acc: 0.0000e+00\n",
      "Epoch 3/4\n",
      "28/27 [==============================] - 0s 10ms/step - loss: -988.4279 - acc: 0.0000e+00 - val_loss: -5.1243 - val_acc: 0.0000e+00\n",
      "Epoch 4/4\n",
      "28/27 [==============================] - 0s 10ms/step - loss: -731.6416 - acc: 0.0000e+00 - val_loss: -7.4018 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e001917f0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_generater = train_generator(x_train, y_train, 250)\n",
    "my_test_generater = train_generator(x_test, y_test, 250)\n",
    "model.fit_generator(my_generater, steps_per_epoch=len(x_train)/50, epochs=4, \n",
    "                     verbose=1, class_weight=class_weights,\n",
    "                    validation_data=my_test_generater, \n",
    "                    validation_steps = len(x_test)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_generater = test_generator(x_test, 250)\n",
    "predictions = model.predict_generator(my_test_generater, steps=len(x_test)/100)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 250 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-69374675834d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 250 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "x_train = x_train[250,:,:,]\n",
    "model.fit(x_train,y_train,epochs=10, batch_size=30, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected conv1d_17_input to have 3 dimensions, but got array with shape (1396, 8, 4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-7fc62e29572d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_compare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_compare\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[1;32m-> 1025\u001b[1;33m                                   steps=steps)\n\u001b[0m\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1822\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1823\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1824\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1825\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected conv1d_17_input to have 3 dimensions, but got array with shape (1396, 8, 4, 1)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_compare = np.argmax(y_test,axis=1)\n",
    "score = accuracy_score(y_compare,pred)\n",
    "cm = confusion_matrix(y_compare,pred)\n",
    "print(cm)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_generater = train_generator(x_train, y_train, WINDOW_SIZE)\n",
    "my_test_generater = train_generator(x_test, y_test, WINDOW_SIZE)\n",
    "model.fit_generator(my_generater, steps_per_epoch=len(x_train)/50, epochs=4, \n",
    "                     verbose=1, \n",
    "                    validation_data=my_test_generater, \n",
    "                    validation_steps = len(x_test)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_test_generater = test_generator(x_test, WINDOW_SIZE)\n",
    "predictions = model.predict_generator(my_test_generater, steps=len(x_test)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0] = 0\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(image_mappping(x_train,500))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = single_metric_auc_score(predictions, y_test, True)\n",
    "print('AUC score: %.3f' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0: \n",
      "   1:\n",
      "\n",
      "from keras.utils import to_categorical\n",
      "from keras.optimizers import Adam, Adadelta\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten, Conv1D\n",
      "from keras.layers.convolutional import MaxPooling2D, MaxPooling1D\n",
      "from keras.models import load_model\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
      "from sklearn.utils import class_weight\n",
      "from scipy import signal\n",
      "from glob import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "   2:\n",
      "# constants:\n",
      "SUBSAMPLE = 100  # scales the data by this factor can be replaced by divding the steps per epoch by batch size\n",
      "COLUMNS = ['HandStart', 'FirstDigitTouch',\n",
      "        'BothStartLoadPhase', 'LiftOff',\n",
      "        'Replace', 'BothReleased']\n",
      "SUBJECTS = range(1, 2)\n",
      "N_LABELS = 6\n",
      "\n",
      "# Below are the paths to the data. Please pay attention to the % and * signs, these are needed in the loop.\n",
      "#TRAIN_DATA_PATH = 'C:/Users/Sebastiaan/Desktop/Programming/MachineLearning/Datasets/EEG/train/subj%d_series*_data.csv'\n",
      "TRAIN_DATA_PATH = 'C:/Users/Gebruiker/Documents/Untitled Folder/train/train/subj%d_series*_data.csv'\n",
      "#The path below is for the test data used for a kaggle submission. This is not very relevant to our project.\n",
      "# TEST_DATA_PATH = 'C:/Users/Sebastiaan/Desktop/Programming/MachineLearning/Datasets/EEG/test/subj%d_series*_data.csv'\n",
      "# TRAIN_DATA_PATH = 'C:/Users/bas/Documents/MachineLearning/train/subj%d_series*_data.csv' #path on my laptop\n",
      "# TEST_DATA_PATH =  'C:/Users/bas/Documents/MachineLearning/test/subj%d_series*_data.csv' \n",
      "\n",
      "SUBMISSION_FOLDER = 'C:/Users/Sebastiaan/Desktop/Programming/MachineLearning/'\n",
      "SUBMISSION_NAME = 'subbmision_vu_48_sub_pca_4.csv'\n",
      "\n",
      "PCA_COMPONENTS = 0.8\n",
      "CUTT_OFF_FREQUENCY = 2\n",
      "ORDER = 4\n",
      "SAMPLE_FREQUENCY = 500\n",
      "\n",
      "EPOCHS = 5\n",
      "WINDOW_SIZE = 500\n",
      "   3:\n",
      "def prepare_training_data(data_path):\n",
      "    data = pd.read_csv(data_path)\n",
      "    event_path = data_path.replace('_data', '_events')\n",
      "    labels = pd.read_csv(event_path)\n",
      "    clean_data = data.drop(['id'], axis = 1)\n",
      "    labels = labels.drop(['id'], axis = 1)\n",
      "    return clean_data, labels\n",
      "   4:\n",
      "def prepare_test_data(data_path):\n",
      "    data = pd.read_csv(data_path)\n",
      "    return data\n",
      "   5:\n",
      "def scaler_transform(data, scaler):\n",
      "    if scaler == None:\n",
      "        scaler = StandardScaler()\n",
      "        return scaler.fit_transform(data), scaler\n",
      "    else:\n",
      "        return scaler.transform(data)\n",
      "   6:\n",
      "def read_training_data(train_data_paths):\n",
      "    labels_raw = []\n",
      "    features_raw = []\n",
      "    for data_path in train_data_paths:\n",
      "        data, labels = prepare_training_data(data_path)\n",
      "        features_raw.append(data[4600:4700])\n",
      "        labels_raw.append(labels[4600:4700])\n",
      "    features_raw = pd.concat(features_raw)\n",
      "    labels_raw = pd.concat(labels_raw)\n",
      "    x_train = np.asarray(features_raw.astype(float))\n",
      "    y_train = np.asarray(labels_raw.astype(float))\n",
      "    return x_train, y_train\n",
      "   7:\n",
      "def read_test_data(test_data_paths):\n",
      "    test_features_raw = []\n",
      "    ids = []\n",
      "    for data_path in test_data_paths:\n",
      "        data = prepare_test_data(data_path)\n",
      "        test_features_raw.append(data)\n",
      "        ids.append(np.array(data['id']))\n",
      "    test_features_raw = pd.concat(test_features_raw)\n",
      "    ids = np.concatenate(ids)\n",
      "    test_features_raw = test_features_raw.drop(['id'], axis = 1)\n",
      "    x_test = np.asarray(test_features_raw.astype(float))\n",
      "    return x_test, ids\n",
      "   8:\n",
      "def metric_auc_score(predictions, y_test, with_plot):\n",
      "    scores = []\n",
      "    legend_text = []\n",
      "    for i in range(N_LABELS):\n",
      "        fpr, tpr, _  = roc_curve(y_test[:,i], predictions[:,i], 1)\n",
      "        scores.append(roc_auc_score(y_test[:,i], predictions[:,i]))\n",
      "        legend_text.append(COLUMNS[i]+' (area = %.3f)' % (scores[i]))\n",
      "        if with_plot == True:\n",
      "            plt.plot(fpr, tpr)\n",
      "    if with_plot == True:\n",
      "        plt.xlabel('False Positive Rate')\n",
      "        plt.ylabel('True Positive Rate')\n",
      "        plt.title('ROC Curves')\n",
      "        plt.legend(legend_text)\n",
      "        plt.show()\n",
      "    return scores\n",
      "   9:\n",
      "def single_metric_auc_score(predictions, y_test, with_plot):\n",
      "    fpr, tpr, _  = roc_curve(y_test, predictions, 1)\n",
      "    score = roc_auc_score(y_test, predictions)\n",
      "    print(COLUMNS[0]+' AUC score = %.3f' % (score))\n",
      "    if with_plot == True:\n",
      "        plt.plot(fpr, tpr)\n",
      "        plt.xlabel('False Positive Rate')\n",
      "        plt.ylabel('True Positive Rate')\n",
      "        plt.title('ROC Curves')\n",
      "        plt.show()\n",
      "    return score\n",
      "  10:\n",
      "def all_auc_scores(prediction_total, test_data_total, subjects, with_plot):\n",
      "    scores = []\n",
      "    for subject in subjects:\n",
      "        score = metric_auc_score(prediction_total[subject-1],\n",
      "                                 test_data_total[subject-1], with_plot)\n",
      "        scores.append(score)\n",
      "        print('Mean AUC Score of Subject %d: %.3f' % \\\n",
      "              (subject, np.mean(score)))\n",
      "    return scores\n",
      "  11:\n",
      "# def image_mappping(x_train, WINDOW_SIZE):\n",
      "#     result = []\n",
      "#     empty_matrix = np.atleast_3d(np.zeros(np.shape(x_train[0:WINDOW_SIZE])))\n",
      "#     for i in range(len(x_train)):\n",
      "#         if i-WINDOW_SIZE < 0:\n",
      "#             result.append(empty_matrix)\n",
      "#         else:\n",
      "#             result.append(np.atleast_3d(x_train[i-WINDOW_SIZE:i]))\n",
      "#     return result\n",
      "\n",
      "def image_mappping(x_train, WINDOW_SIZE):\n",
      "    result = []\n",
      "    empty_matrix = np.zeros(np.shape(x_train[0:WINDOW_SIZE]))\n",
      "    for i in range(len(x_train)):\n",
      "        if i-WINDOW_SIZE < 0:\n",
      "            result.append(empty_matrix)\n",
      "        else:\n",
      "            result.append(x_train[i-WINDOW_SIZE:i])\n",
      "    return result\n",
      "  12:\n",
      "def train_generator(x_train, y_train, WINDOW_SIZE):\n",
      "    x = image_mappping(x_train, WINDOW_SIZE)\n",
      "    while True:\n",
      "        for image, task in zip(x, y_train):\n",
      "            yield np.array([image]), np.array([task])\n",
      "  13:\n",
      "def test_generator(x_test, WINDOW_SIZE):\n",
      "    x = image_mappping(x_test, WINDOW_SIZE)\n",
      "    while True:\n",
      "        for image in x:\n",
      "            yield np.array([image])\n",
      "  14:\n",
      "def simple_val(vals):\n",
      "    new = np.empty([len(vals),1])\n",
      "    for i in range(0,len(vals)):\n",
      "        value = vals[i]\n",
      "        for index in range(0,len(value)):\n",
      "            if index == 1:\n",
      "                new[i] = index\n",
      "                break\n",
      "    return new\n",
      "  15:\n",
      "def class_weights(y_train):\n",
      "    class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train[:,0]), y_train[:,0])\n",
      "    return {0 : class_weight[1], 2: class_weight[0]}\n",
      "  16:\n",
      "def init_cnn(window):\n",
      "    \"\"\"model = Sequential((\n",
      "    # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
      "    # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
      "    # the input timeseries, the activation of each filter at that position.\n",
      "    Conv1D(nb_filter=50, filter_length=250, activation='relu', input_shape=(window, 32)),\n",
      "    MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
      "    Conv1D(nb_filter=50, filter_length=100, activation='relu'),\n",
      "    MaxPooling1D(),\n",
      "    Flatten(),\n",
      "    Dense(1, activation='softmax'),    \n",
      "    ))\n",
      "    optimizer = Adadelta()\n",
      "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    model.summary()\"\"\"\n",
      "\n",
      "    \n",
      "    model = Sequential()\n",
      "    model.add(Conv2D(16, kernel_size=(2, 2), activation='relu', input_shape=(8,4,1)))\n",
      "    model.add(Conv2D(32, (2, 2), activation='relu'))\n",
      "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
      "    model.add(Dropout(0.25)) # Dropout 25% of the nodes of the previous layer during training\n",
      "    model.add(Flatten())     # Flatten, and add a fully connected layer\n",
      "    model.add(Dense(128, activation='relu')) \n",
      "    model.add(Dropout(0.5))\n",
      "    model.add(Dense(6, activation='softmax')) # Last layer: 10 class nodes, with dropout\n",
      "    model.summary()\n",
      "    optimizer = Adadelta()\n",
      "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    return model\n",
      "  17:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    print(simple_vals(y_train))\n",
      "  18:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    print(simple_val(y_train))\n",
      "  19:\n",
      "def simple_val(vals):\n",
      "    new = np.empty([len(vals),1])\n",
      "    for i in range(0,len(vals)):\n",
      "        value = vals[i]\n",
      "        for index in range(0,len(value)):\n",
      "            if value == 1:\n",
      "                new[i] = index\n",
      "                break\n",
      "            elif index == len(value):\n",
      "                new[i] = 0\n",
      "                break\n",
      "    return new\n",
      "  20:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    print(simple_val(y_train))\n",
      "  21:\n",
      "def simple_val(vals):\n",
      "    new = np.empty([len(vals),1])\n",
      "    for i in range(0,len(vals)):\n",
      "        value = vals[i]\n",
      "        for index in range(0,len(value)):\n",
      "            if value[index] == 1:\n",
      "                new[i] = index\n",
      "                break\n",
      "            elif index == len(value):\n",
      "                new[i] = 0\n",
      "                break\n",
      "    return new\n",
      "  22:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    print(simple_val(y_train))\n",
      "  23:\n",
      "def simple_val(vals):\n",
      "    new = np.empty([len(vals),1])\n",
      "    for i in range(0,len(vals)):\n",
      "        value = vals[i]\n",
      "        for index in range(0,len(value)):\n",
      "            if value[index] == 1:\n",
      "                new[i] = index\n",
      "                print(new[i])\n",
      "                break\n",
      "            elif index == len(value):\n",
      "                new[i] = 0\n",
      "                break\n",
      "    return new\n",
      "  24:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    #print(simple_val(y_train))\n",
      "  25:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    simple_val(y_train)\n",
      "    #print(simple_val(y_train))\n",
      "  26:\n",
      "def simple_val(vals):\n",
      "    new = np.empty([len(vals),1])\n",
      "    for i in range(0,len(vals)):\n",
      "        value = vals[i]\n",
      "        for index in range(0,len(value)):\n",
      "            if value[index] == 1:\n",
      "                new[i] = index+1\n",
      "                break\n",
      "            elif index == len(value):\n",
      "                new[i] = 0\n",
      "                break\n",
      "    print(new)\n",
      "    return new\n",
      "  27:\n",
      "def class_weights(y_train):\n",
      "    class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train[:,0]), y_train[:,0])\n",
      "    return {0 : class_weight[1], 2: class_weight[0]}\n",
      "  28:\n",
      "def init_cnn(window):\n",
      "    \"\"\"model = Sequential((\n",
      "    # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
      "    # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
      "    # the input timeseries, the activation of each filter at that position.\n",
      "    Conv1D(nb_filter=50, filter_length=250, activation='relu', input_shape=(window, 32)),\n",
      "    MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
      "    Conv1D(nb_filter=50, filter_length=100, activation='relu'),\n",
      "    MaxPooling1D(),\n",
      "    Flatten(),\n",
      "    Dense(1, activation='softmax'),    \n",
      "    ))\n",
      "    optimizer = Adadelta()\n",
      "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    model.summary()\"\"\"\n",
      "\n",
      "    \n",
      "    model = Sequential()\n",
      "    model.add(Conv2D(16, kernel_size=(2, 2), activation='relu', input_shape=(8,4,1)))\n",
      "    model.add(Conv2D(32, (2, 2), activation='relu'))\n",
      "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
      "    model.add(Dropout(0.25)) # Dropout 25% of the nodes of the previous layer during training\n",
      "    model.add(Flatten())     # Flatten, and add a fully connected layer\n",
      "    model.add(Dense(128, activation='relu')) \n",
      "    model.add(Dropout(0.5))\n",
      "    model.add(Dense(6, activation='softmax')) # Last layer: 10 class nodes, with dropout\n",
      "    model.summary()\n",
      "    optimizer = Adadelta()\n",
      "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    return model\n",
      "  29:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    simple_val(y_train)\n",
      "    #print(simple_val(y_train))\n",
      "  30:\n",
      "def simple_val(vals):\n",
      "    new = np.empty([len(vals),1])\n",
      "    for i in range(0,len(vals)):\n",
      "        value = vals[i]\n",
      "        for index in range(0,len(value)):\n",
      "            if value[index] == 1:\n",
      "                new[i] = index+1\n",
      "                print(new)\n",
      "                break\n",
      "            elif index == len(value):\n",
      "                new[i] = 0\n",
      "                print(new)\n",
      "                break\n",
      "    return new\n",
      "  31:\n",
      "def class_weights(y_train):\n",
      "    class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train[:,0]), y_train[:,0])\n",
      "    return {0 : class_weight[1], 2: class_weight[0]}\n",
      "  32:\n",
      "def init_cnn(window):\n",
      "    \"\"\"model = Sequential((\n",
      "    # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
      "    # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
      "    # the input timeseries, the activation of each filter at that position.\n",
      "    Conv1D(nb_filter=50, filter_length=250, activation='relu', input_shape=(window, 32)),\n",
      "    MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
      "    Conv1D(nb_filter=50, filter_length=100, activation='relu'),\n",
      "    MaxPooling1D(),\n",
      "    Flatten(),\n",
      "    Dense(1, activation='softmax'),    \n",
      "    ))\n",
      "    optimizer = Adadelta()\n",
      "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    model.summary()\"\"\"\n",
      "\n",
      "    \n",
      "    model = Sequential()\n",
      "    model.add(Conv2D(16, kernel_size=(2, 2), activation='relu', input_shape=(8,4,1)))\n",
      "    model.add(Conv2D(32, (2, 2), activation='relu'))\n",
      "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
      "    model.add(Dropout(0.25)) # Dropout 25% of the nodes of the previous layer during training\n",
      "    model.add(Flatten())     # Flatten, and add a fully connected layer\n",
      "    model.add(Dense(128, activation='relu')) \n",
      "    model.add(Dropout(0.5))\n",
      "    model.add(Dense(6, activation='softmax')) # Last layer: 10 class nodes, with dropout\n",
      "    model.summary()\n",
      "    optimizer = Adadelta()\n",
      "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
      "    return model\n",
      "  33:\n",
      "for subject in SUBJECTS:\n",
      "    test_features_raw = []\n",
      "    train_data_paths = glob(TRAIN_DATA_PATH % (subject))\n",
      "  \n",
      "    x_train, y_train = read_training_data(train_data_paths) \n",
      "    #y_train = np.random.randint(2,size=len(y_train))\n",
      "    #print(y_train)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5)\n",
      "    print(y_train.shape)\n",
      "    x_train = x_train.reshape([400,8,4])[:,:,:,None]\n",
      "    simple_val(y_train)\n",
      "    #print(simple_val(y_train))\n",
      "  34: %history -n -o 12-17\n",
      "  35: %history -n -o 0-30\n",
      "  36: %history -n -o 0-50\n"
     ]
    }
   ],
   "source": [
    "%history -n -o 0-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
